{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Dense, Dropout, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(Model):\n",
    "    def __init__(self, model_dim):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape[1]\n",
    "        position_encodings = np.zeros((seq_length, self.model_dim))\n",
    "        for pos in range(seq_length):\n",
    "            for i in range(self.model_dim):\n",
    "                position_encodings[pos, i] = pos / np.power(10000, (i-i%2) / self.model_dim)\n",
    "        position_encodings[:, 0::2] = np.sin(position_encodings[:, 0::2])\n",
    "        position_encodings[:, 1::2] = np.cos(position_encodings[:, 1::2])\n",
    "        position_encodings = tf.expand_dims(tf.cast(position_encodings, tf.float32), axis=0)\n",
    "        return position_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Model):\n",
    "    def __init__(self, masking=True, future=False, dropout=0):\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.future = future\n",
    "        self.dropout = dropout\n",
    "        self.masking_num = -2**32+1\n",
    "        \n",
    "        \n",
    "    def mask(self, inputs, masks):\n",
    "        masks = tf.cast(masks, tf.float32)\n",
    "        masks = tf.tile(masks, [tf.shape(inputs)[0] // tf.shape(masks)[0], 1])\n",
    "        masks = tf.expand_dims(masks, axis=1)\n",
    "        outputs = inputs + masks * self.masking_num\n",
    "        return outputs\n",
    "    \n",
    "    def future_mask(self, inputs):\n",
    "        diag_vals = tf.ones_like(inputs[0])\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "        future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])\n",
    "        paddings = tf.ones_like(future_masks) * self.masking_num\n",
    "        outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)\n",
    "        return outputs\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if self.masking:\n",
    "            tf.assert_equal(len(inputs), 4)\n",
    "            queries, keys, values, masks = inputs\n",
    "        else:\n",
    "            tf.assert_equal(len(inputs), 3)\n",
    "            queries, keys, values = inputs\n",
    "        \n",
    "        # dtype\n",
    "        if queries.dtype != tf.float32:\n",
    "            queries = tf.cast(queries, tf.float32)\n",
    "        if keys.dtype != tf.float32:\n",
    "            keys = tf.cast(keys, tf.float32)\n",
    "        if values.dtype != tf.float32:\n",
    "            values = tf.cast(values, tf.float32)\n",
    "        \n",
    "        matmul = tf.matmul(queries, tf.transpose(keys, [0, 2, 1]))\n",
    "        scaled_matmul = matmul / tf.sqrt(tf.cast(queries.shape[-1], tf.float32))\n",
    "        if self.masking:\n",
    "            scaled_matmul = self.mask(scaled_matmul, masks)\n",
    "        if self.future:\n",
    "            scaled_matmul = self.future_mask(scaled_matmul)\n",
    "        \n",
    "        softmax_out = tf.nn.softmax(scaled_matmul)\n",
    "        \n",
    "        out = tf.nn.dropout(softmax_out, self.dropout)\n",
    "        outputs = tf.matmul(out, values)\n",
    "        \n",
    "        return outputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Model):\n",
    "    def __init__(self, n_heads, head_dim, dropout=.1, masking=True, \n",
    "                 future=False, trainable=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.dropout = dropout\n",
    "        self.masking = masking\n",
    "        self.future = future\n",
    "        self.trainable = trainable\n",
    "        self.q_dense = Dense(head_dim*n_heads)\n",
    "        self.k_dense = Dense(head_dim*n_heads)\n",
    "        self.v_dense = Dense(head_dim*n_heads)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if self.masking:\n",
    "            tf.assert_equal(len(inputs), 4)\n",
    "            queries, keys, values, masks = inputs\n",
    "        else:\n",
    "            tf.assert_equal(len(inputs), 3)\n",
    "            queries, keys, values = inputs\n",
    "        \n",
    "        queries_linear = self.q_dense(queries)\n",
    "        keys_linear = self.k_dense(keys)\n",
    "        values_linear = self.v_dense(values)\n",
    "        \n",
    "        queries_multi_heads = tf.concat(tf.split(queries_linear, self.n_heads, axis=2), axis=0)\n",
    "        keys_multi_heads = tf.concat(tf.split(keys_linear, self.n_heads, axis=2), axis=0)\n",
    "        values_multi_heads = tf.concat(tf.split(values_linear, self.n_heads, axis=2), axis=0)\n",
    "        \n",
    "        if self.masking:\n",
    "            att_inputs = [queries_multi_heads, keys_multi_heads, values_multi_heads, masks]\n",
    "        else:\n",
    "            att_inputs = [queries_multi_heads, keys_multi_heads, values_multi_heads]\n",
    "        \n",
    "        attention = ScaledDotProductAttention(masking=self.masking, future=self.future, \n",
    "                                             dropout=self.dropout)\n",
    "        att_out = attention(att_inputs)\n",
    "#         print(att_out.shape)\n",
    "        \n",
    "        outputs = tf.concat(tf.split(att_out, self.n_heads, axis=0), axis=2)\n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 512) (1, 256, 512)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "max_len = 256\n",
    "model_dim = 512\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(maxlen=max_len, num_words=vocab_size)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "x_train_masks = tf.equal(x_train, 0)\n",
    "x_test_masks = tf.equal(x_test, 0)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "inputs = Input(shape=(max_len,), name='inputs')\n",
    "masks = Input(shape=(max_len,), name='masks')\n",
    "embeddings = Embedding(vocab_size, model_dim)(inputs)\n",
    "encodings = PositionEncoding(model_dim)(embeddings)\n",
    "print(embeddings.shape, encodings.shape)\n",
    "encodings = tf.keras.layers.add([embeddings, encodings])\n",
    "x = MultiHeadAttention(8, 64)([encodings, encodings, encodings, masks])\n",
    "x = GlobalAveragePooling1D(data_format='channels_last')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(10, activation='relu')(x)\n",
    "outputs = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=[inputs, masks], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 256, 512)     2560000     inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "position_encoding (PositionEnco (1, 256, 512)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256, 512)     0           embedding[0][0]                  \n",
      "                                                                 position_encoding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "masks (InputLayer)              [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 256, 512)     787968      add[0][0]                        \n",
      "                                                                 add[0][0]                        \n",
      "                                                                 add[0][0]                        \n",
      "                                                                 masks[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 512)          0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           5130        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            22          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,353,120\n",
      "Trainable params: 3,353,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training ... \n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.6943 - accuracy: 0.5041 - val_loss: 0.6934 - val_accuracy: 0.4916\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6936 - val_accuracy: 0.4916\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6932 - val_accuracy: 0.4916\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6931 - accuracy: 0.5027 - val_loss: 0.6935 - val_accuracy: 0.4916\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6934 - val_accuracy: 0.4916\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6936 - val_accuracy: 0.4916\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6935 - val_accuracy: 0.4916\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 27s 1ms/sample - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6936 - val_accuracy: 0.4916\n",
      "loss on Test: 0.6935\n",
      "accu on Test: 0.4936\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9), \n",
    "    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Training ... \")\n",
    "es = EarlyStopping(patience=5)\n",
    "model.fit([x_train, x_train_masks], y_train, \n",
    "          batch_size=batch_size, epochs=epochs, \n",
    "          validation_split=0.2, callbacks=[es])\n",
    "\n",
    "test_metrics = model.evaluate([x_test, x_test_masks], y_test, batch_size=batch_size, verbose=0)\n",
    "print(\"loss on Test: %.4f\" % test_metrics[0])\n",
    "print(\"accu on Test: %.4f\" % test_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(Model):\n",
    "    def __init__(self, model_dim, inner_dim, trainable=True):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.inner_dim = inner_dim\n",
    "        self.trainable = trainable\n",
    "        self.dense0 = Dense(inner_dim, activation='relu')\n",
    "        self.dense1 = Dense(model_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inner_out = self.dense0(inputs)\n",
    "        outputs = self.dense1(inner_out)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Model):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = BatchNormalization(epsilon=self.epsilon)(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, vocab_size, model_dim, n_heads=8, encoder_stack=6, \n",
    "                decoder_stack=6, feed_forward_size=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_dim = model_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.encoder_stack = encoder_stack\n",
    "        self.decoder_stack = decoder_stack\n",
    "        self.feed_forward_size = feed_forward_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding_layer = Embedding(vocab_size, model_dim)\n",
    "        \n",
    "    def encoder(self, inputs):\n",
    "        masks = tf.equal(inputs, 0)\n",
    "        embeddings = self.embedding_layer(inputs)\n",
    "        position_encodings = PositionEncoding(self.model_dim)(embeddings)\n",
    "        position_encodings = tf.tile(position_encodings, [tf.shape(inputs)[0], 1, 1])\n",
    "        print(embeddings.shape, position_encodings.shape)\n",
    "        encodings = tf.keras.layers.add([embeddings, position_encodings])\n",
    "        encodings = tf.nn.dropout(encodings, self.dropout)\n",
    "        \n",
    "        for i in range(self.encoder_stack):\n",
    "            attention = MultiHeadAttention(self.n_heads, \n",
    "                                           self.model_dim // self.n_heads)\n",
    "            attention_input = [encodings, encodings, encodings, masks]\n",
    "            attention_out = attention(attention_input)\n",
    "            attention_out = tf.keras.layers.add([attention_out, encodings])\n",
    "            attention_out = LayerNormalization()(attention_out)\n",
    "#             print(attention_out.shape)\n",
    "            ff = PositionWiseFeedForward(self.model_dim, self.feed_forward_size)\n",
    "            ff_out = ff(attention_out)\n",
    "#             print(ff_out.shape)\n",
    "            ff_out = tf.keras.layers.add([ff_out, attention_out])\n",
    "            encodings = LayerNormalization()((ff_out))\n",
    "        \n",
    "        return encodings, masks\n",
    "        \n",
    "    def decoder(self, inputs):\n",
    "        decoder_inputs, encoder_encodings, encoder_masks = inputs\n",
    "        \n",
    "        decoder_masks = tf.equal(decoder_inputs, 0)\n",
    "        embeddings = self.embedding_layer(decoder_inputs)\n",
    "        position_encodings = PositionEncoding(self.model_dim)(embeddings)\n",
    "        encodings = embeddings + position_encodings\n",
    "        encodings = tf.nn.dropout(encodings, self.dropout)\n",
    "        \n",
    "        for i in range(self.decoder_stack):\n",
    "            masked_attention = MultiHeadAttention(self.n_heads, \n",
    "                                                  self.model_dim // self.n_heads, \n",
    "                                                 future=True)\n",
    "            masked_attention_input = [encodings, encodings, encodings, decoder_masks]\n",
    "            masked_attention_out = masked_attention(masked_attention_input)\n",
    "            \n",
    "            masked_attention_out = tf.keras.layers.add([masked_attention_out, encodings])\n",
    "            masked_attention_out = LayerNormalization()(masked_attention_out)\n",
    "            \n",
    "            attention = MultiHeadAttention(self.n_heads, \n",
    "                                          self.model_dim // self.n_heads)\n",
    "            attention_input = [masked_attention_out, encoder_encodings, \n",
    "                               encoder_encodings, encoder_masks]\n",
    "            attention_out = attention(attention_input)\n",
    "            attention_out = tf.keras.layers.add([attention_out, masked_attention_out])\n",
    "            attention_out = LayerNormalization()(attention_out)\n",
    "            \n",
    "            ff = PositionWiseFeedForward(self.model_dim, self.feed_forward_size)\n",
    "            ff_out = ff(attention_out)\n",
    "            ff_out = tf.keras.layers.add([ff_out, attention_out])\n",
    "            encodings = LayerNormalization()(ff_out)\n",
    "            \n",
    "        \n",
    "        linear_projection = tf.matmul(encodings, tf.transpose(self.embedding_layer.weights, perm=[0, 2, 1]))\n",
    "        outputs = tf.nn.softmax(linear_projection)\n",
    "        return outputs\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        encoder_encodings, encoder_masks = self.encoder(encoder_inputs)\n",
    "        decoder_outputs = self.decoder([decoder_inputs, encoder_encodings, \n",
    "                                       encoder_masks])\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 512) (None, 256, 512)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "max_len = 256\n",
    "model_dim = 512\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(maxlen=max_len, num_words=vocab_size)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "x_train_masks = tf.equal(x_train, 0)\n",
    "x_test_masks = tf.equal(x_test, 0)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len,), name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(max_len,), name='decoder_inputs')\n",
    "outputs = Transformer(vocab_size, model_dim)([encoder_inputs, decoder_inputs])\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# outputs = GlobalAveragePooling1D(data_format='channels_first')(outputs)\n",
    "# outputs = Dropout(0.5)(outputs)\n",
    "# outputs = Dense(128, activation='relu')(outputs)\n",
    "# outputs = Dense(2, activation='softmax')(outputs)\n",
    "# model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_20 (Transformer)    (None, 256, 5000)    2560000     encoder_inputs[0][0]             \n",
      "                                                                 decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 256)          0           transformer_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256)          0           global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 128)          32896       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 2)            258         dense_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,593,154\n",
      "Trainable params: 2,593,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9), \n",
    "#     loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print(\"Model Training ... \")\n",
    "# es = EarlyStopping(patience=5)\n",
    "# model.fit([x_train, x_train_masks], y_train, \n",
    "#           batch_size=batch_size, epochs=epochs, \n",
    "#           validation_split=0.2, callbacks=[es])\n",
    "\n",
    "# test_metrics = model.evaluate([x_test, x_test_masks], y_test, batch_size=batch_size, verbose=0)\n",
    "# print(\"loss on Test: %.4f\" % test_metrics[0])\n",
    "# print(\"accu on Test: %.4f\" % test_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
